{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll go beyond standard data analysis and actually code a machine learning algorithm from scratch. As this is our first such exercise we'll start with a classic linear model, Logistic Regression. We'll accomplish this with the following steps:\n",
    "\n",
    "1. Build a class that can fit a Logistic Regression given training data as well as make predictions on examples without labels\n",
    "2. Write a unit test that confirms our class produces the correct estimates on synthetically generated data\n",
    "3. Benchmark our class against SkLearn for speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Logistic Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any programming, we need to first understand the optimization algorithm we'll use. As a reference, we'll be following the presentation presented here: https://tminka.github.io/papers/logreg/minka-logreg-old.pdf.\n",
    "\n",
    "The logistic loss is convex so we'll use a convex optimization process called gradient descent. In particular, we will use a variant called Newton's Method that uses the 2nd derivative of the loss function to set the learning rate. To start, here are some primitives:\n",
    "\n",
    "<b>$L = \\sum y * ln(p) + (1-y)*ln(1-p)$</b>\n",
    "\n",
    "(Note, we'll drop subscripts over the data instances to keep notation simple. All sums are over examples in the data set.)\n",
    "\n",
    "<b> $g_j = \\nabla L_j = \\sum (y-p)*x_j$</b>\n",
    "\n",
    "(This is the 1st derivative, where $j$ indicates feature dimension $j$)\n",
    "\n",
    "<b> $H_{jk} = \\sum p*(1-p)*x_j*x_k$</b>\n",
    "\n",
    "(This is the 2nd derivative w.r.t. features $j$ and $k$)\n",
    "\n",
    "Now the general form of Gradient Descent follows this function:\n",
    "\n",
    "<b>$w_{new} = w_{old} - \\nu * g$</b>\n",
    "\n",
    "Where $w$ is the weight and $\\nu$ is an appropriately chosen step size. In our case, we're going to use the inverse of the 2nd derviative matrix (the Hessian) as our learning rate. If we define $H$ as the Hessian matrix with entries $H_{jk}$ from above, and $G$ a gradient vector who's $jth$ entry is $g_j$, then our optimization problem becomes:\n",
    "\n",
    "<b>$w_{new} = w_{old} - H^{-1} * G$</b>\n",
    "\n",
    "This is what we are going to program here. Let's define a class that has the following methods:\n",
    "\n",
    "1. An __init__ method that takes in an error tolerence as a stopping criterion, as well as max number of iterations.\n",
    "2. A <b>predict</b> method that takes a given matrix X and predicts $p=(1+e^{(-X*B)})^{-1}$ for each entry\n",
    "3. A <b>compute_gradient</b> method that computes the gradient vector $G$\n",
    "4. A <b>compute_hessian</b> method that computes the Hessian. Note that the $H$ can be broken down to the following matrix multiplcation: $H=X^TQX$, where $X$ is the input matrix and $Q$ is a diagonal matrix where each entry $Q_{ii}=p_i*(1-p_i)$.\n",
    "5. An <b>update_weights</b> method that applies Newton's method to update the weights\n",
    "6. A <b>check_stop</b> method that checks whether the model has converged or the max iterations have been met\n",
    "7. A <b>fit</b> method that takes in the data and runs the gradient optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a class with the following API\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyFirstLogReg(object):\n",
    "    \n",
    "    def __init__(self, tol = 10**-8, max_iterations = 20):\n",
    "        \n",
    "        #Initialize the variables\n",
    "        \n",
    "        self.tolerance = #Student enter code here\n",
    "        self.max_iterations = #Student enter code here\n",
    "        self.beta = #Student enter code here\n",
    "        self.alpha = #Student enter code here\n",
    "        \n",
    "    def predict(self, Xint):\n",
    "        '''\n",
    "        Compute probs using the inverse logit\n",
    "        - Inputs: The NxK X matrix\n",
    "        - Outputs: Vector of probs of length N\n",
    "        '''\n",
    "        \n",
    "        #First compute X*beta+alpha\n",
    "        #Student enter code here \n",
    "        #Student enter code here\n",
    "        return P\n",
    "        \n",
    "    def compute_gradient(self, Xint, y, p):\n",
    "        '''\n",
    "        Computes the gradient vector\n",
    "        -Inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 y (label) vector\n",
    "            - Nx1 ps vector of predictions\n",
    "        -Outputs: 1xK vector of gradients\n",
    "        '''\n",
    "        #Student enter code here\n",
    "        return grad\n",
    "        \n",
    "    def compute_hessian(self, Xint, P):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 vector of predictions\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "        #Note, in first \n",
    "        #Student enter code here\n",
    "        #Student enter code here\n",
    "        return H\n",
    "\n",
    "\n",
    "    def update_weights(self, Xint, y, i):\n",
    "        '''\n",
    "        Updates existing weight vector\n",
    "        -Inputs:\n",
    "            -NxK X matrix\n",
    "            -Nx1 y vector\n",
    "        -updates weights by calling predict, compute_gradient and compute_hessian\n",
    "        '''\n",
    "        #Student enter code here\n",
    "        #Student enter code here\n",
    "        #Student enter code here\n",
    "                \n",
    "        #Store the current weights before updating so we can check for convergence\n",
    "        #Student enter code here\n",
    "        \n",
    "        #update the weights\n",
    "        #Student enter code here\n",
    "        \n",
    "        \n",
    "    def check_stop(self):\n",
    "        '''\n",
    "        check to see if euclidean distance between old and new weights (normalized)\n",
    "        is less than the tolerance\n",
    "        \n",
    "        returns: True or False on whether stopping criteria is met\n",
    "        '''\n",
    "        b_old_norm = #Student enter code here\n",
    "        b_new_norm = #Student enter code here\n",
    "        diff = #Student enter code here\n",
    "        isConverged = #Student enter code here\n",
    "        return isConverged\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X is the Nx(K-1) data matrix\n",
    "        Y is the labels, using {0,1} coding\n",
    "        '''\n",
    "        \n",
    "        #set initial weights - add an extra dimension for the intercept\n",
    "        #Student enter code here\n",
    "        \n",
    "        #Initialize the slope parameter to log(base rate/(1-base rate))\n",
    "        #Student enter code here\n",
    "        \n",
    "        #create a new X matrix that includes a column of ones for the intercept\n",
    "        #Student enter code here\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            self.update_weights(#Student enter code here)\n",
    "            self.beta = #Student enter code here\n",
    "            self.alpha = #Student enter code here\n",
    "            if self.check_stop():\n",
    "                break\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note about testing\n",
    "\n",
    "One way we can test this implementation is to generate some random data according to a logistic model, and see if our model returns the correct weights. To do this:\n",
    "\n",
    "\n",
    "* generate an NxK X matrix of random numbers\n",
    "* generate a 1xK weight vector called Beta\n",
    "* set alpha\n",
    "* Given Beta and Alpha, compute P(Y|X) using the logistic function\n",
    "* Generate an Nx1 random sequence in [0,1], and set Y=1 if R_i < P_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate a dataset with N=10k and K=5\n",
    "def gen_logistic(N, K, Beta, Alpha):\n",
    "    X = #Student enter code here\n",
    "    XB = #Student enter code here\n",
    "    P = #Student enter code here\n",
    "    Y = #Student enter code here\n",
    "    return X, Y\n",
    "\n",
    "K = 2\n",
    "\n",
    "Beta = 2*(np.random.random(K)-1)\n",
    "Alpha = -1\n",
    "\n",
    "X, Y = gen_logistic(1000, K, Beta, Alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's test out our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = MyFirstLogReg()\n",
    "lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The real Betas and Alpha are:')\n",
    "print(Beta, Alpha)\n",
    "print('')\n",
    "print('The fitted Betas and Alpha are:')\n",
    "print(lr.beta, lr.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's compare our fitted results to SkLearn's Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_SK = #Student enter code here\n",
    "LR_SK.coef_, LR_SK.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is pretty close, though not exact at all digits of precision.  We can also see that both systems produce estimates that are far from the truth. Does this mean that the code is correct? If it is indeed correct, why might we observe the above? We can write a more robust test by running multiple draws, and seeing if on average we get the right answer. We can also increase the sample size. Doing either will be more computationally expensive. Before going there let's profile our code to see if there is a way to optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "cProfile.run(#Student enter code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What method is taking the most amount of time? Looking more closely at the exact sequence of operations, what might be taking up a lot of memory or time? How can we achieve the same mathematic results, but do it in a more memory friendly way?\n",
    "\n",
    "Let's do a quick test, where we design an alternative, and hopefully faster way to compute the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a good place to break. Part 2 in the next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hessian_slow(X, P):\n",
    "    '''\n",
    "    Copy the operations used in the class above\n",
    "    '''\n",
    "    Q = #Student enter code here\n",
    "    H = #Student enter code here\n",
    "    return H\n",
    "    \n",
    "    \n",
    "def hessian_fast(X, P):\n",
    "    '''\n",
    "    Rewrite this without using the np.diag function\n",
    "    '''\n",
    "    Q = #Student enter code here\n",
    "    XQ = #Student enter code here\n",
    "    H = #Student enter code here\n",
    "    return #Student enter code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = 0.5 * np.ones(X.shape[0])\n",
    "%timeit hessian_slow(X, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit hessian_fast(X, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make a difference? Now create a new class, same as above, but overwrite the compute_hessian method with the above faster version. We're going to do this in a very light and efficient way. Essentially we'll inherit MyFirstLogReg class, which means all of the methods in the base class are callable for this one. We can overwrite the compute_hessian method with our faster approach (note, in normal software development we'd likely just revise the original class, but we're doing it this way to show the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyFasterFirstLogReg(MyFirstLogReg):\n",
    "    \n",
    "    def compute_hessian(self, Xint, p):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 vector of predictions\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "        Q = #Student enter code here\n",
    "        XintQ = #Student enter code here\n",
    "        H = #Student enter code here\n",
    "        return H\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now rerun\n",
    "lr = MyFasterFirstLogReg()\n",
    "lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.beta, lr.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last speed test, let's compare our faster class to sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit LogisticRegression().fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit MyFasterFirstLogReg().fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got an efficient class written, let's perform 3 different unit tests:\n",
    "* Compare the results to SkLearn. This replicates a scenario where for some reason we can't use the SkLearn package and had to write our own code. Since SkLearn does exist, we can at least benchmark against it.\n",
    "* Run it once just on a much larger data set (which should come close to the truth value).\n",
    "* Run it many times over different draws from the same data generating distribution and look at the distribution of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's generate a dataset with 1000000 examples\n",
    "K = 4\n",
    "Beta = 2*(np.random.random(K)-1)\n",
    "Alpha = -2\n",
    "\n",
    "X, Y = gen_logistic(1000000, K, Beta, Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR_SK = LogisticRegression(C=10**30).fit(X,Y)\n",
    "LR_Mine = MyFasterFirstLogReg()\n",
    "LR_Mine.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Truth: beta=' + str(LR_SK.coef_) + ', Alpha=' +str(Alpha))\n",
    "print('SkLearn: beta=' + str(LR_SK.coef_) + ', Alpha=' +str(Alpha))\n",
    "print('Ours: beta=' + str(LR_Mine.beta) + ', Alpha=' +str(LR_Mine.alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test we'll check whether on average our own LR class is correct. To do this, run a loop where in each iteration, generate a random data set of size N, using the same Beta and Alpha. Then fit the model and store the weights. After this runs we can look at the distributions of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas = []\n",
    "alphas = []\n",
    "\n",
    "for i in range(1000):\n",
    "    X, Y = #Student enter code here\n",
    "    LR_Mine = #Student enter code here\n",
    "    LR_Mine.fit(#Student enter code here)\n",
    "    betas.append(#Student enter code here)\n",
    "    alphas.append(#Student enter code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the histograms of each parameter distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First put it all in a dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(#Student enter code here)\n",
    "df['alpha'] = #Student enter code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's check the means against the truth\n",
    "df.mean(), Beta, Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "bmeans = df.mean()\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for i in range(K):\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.hist(#Student enter code here)\n",
    "    plt.plot()\n",
    "    plt.title('b {}: Fit={}, Truth={}'.format(i, np.round(bmeans[i],2), np.round(Beta[i],2)))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
